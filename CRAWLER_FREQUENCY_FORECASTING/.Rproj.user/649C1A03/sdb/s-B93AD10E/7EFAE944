{
    "contents" : "% Logs frequency single URL solde number of visits desktop bot forecasting\n% S.Duprey\n% July 15th, 2015\n\n### Loading and preprocessing the logs data ( single URL soldes number of visits per hour google bot / Internet Users )\n```{r data-preprocessing}\nmy_path <- \"D:\\\\My_Data\\\\My_Crawler_Frequency\\\\soldes.log\"\ncrawling_events <- read.csv(file=my_path,header=FALSE, sep=\"|\")\n\n# PREPROCESSING THE DATA\n# Filtering the data to split internet users from google bots\nrequire(stringr)\n\nmy_google_filter = str_detect(crawling_events$V11,ignore.case(\"googlebot\"))\nmy_blank_filter = crawling_events$V11 == \"\"\n\nfiltered_crawling_events<-crawling_events[my_google_filter ,]\ninternautes_filtered_crawling_events<-crawling_events[!my_google_filter & !my_blank_filter ,]\n\nnames(filtered_crawling_events) <- c(\"Timestamp\",\"Hit\",\"N1\",\"N2\",\"N3\",\"N4\",\"N5\",\"N6\",\"TrackedUrl\",\"N7\",\"UserAgent\" )\nnames(internautes_filtered_crawling_events) <- c(\"Timestamp\",\"Hit\",\"N1\",\"N2\",\"N3\",\"N4\",\"N5\",\"N6\",\"TrackedUrl\",\"N7\",\"UserAgent\" )\n\n\nfiltered_crawling_events$Timestamp <- str_sub(string=filtered_crawling_events$Timestamp, start=31, end=50)\ninternautes_filtered_crawling_events$Timestamp <- str_sub(string=internautes_filtered_crawling_events$Timestamp, start=31, end=50)\n\nfiltered_crawling_events$Timestamp <- as.POSIXct(filtered_crawling_events$Timestamp)\ninternautes_filtered_crawling_events$Timestamp <- as.POSIXct(internautes_filtered_crawling_events$Timestamp)\n\n\nfiltered_crawling_events$numeric_timestamp = as.numeric(filtered_crawling_events$Timestamp)\ninternautes_filtered_crawling_events$numeric_timestamp = as.numeric(internautes_filtered_crawling_events$Timestamp)\n\n\n# getting the numeric timestamp passing time\nmy_google_numeric_vector = filtered_crawling_events$numeric_timestamp\nmy_internautes_numeric_vector = internautes_filtered_crawling_events$numeric_timestamp\n\n# sorting the arriving timestamps\nmy_google_numeric_vector = sort(my_google_numeric_vector, decreasing = FALSE)\nmy_internautes_numeric_vector = sort(my_internautes_numeric_vector, decreasing = FALSE)\n\n# no need to visualize the data\n####plot(my_google_numeric_vector, rep(1, NROW(my_google_numeric_vector)))\n####plot(my_internautes_numeric_vector, rep(1, NROW(my_internautes_numeric_vector)))\n```\n\n### Meshing time and computing the number of visits per mesh for internet users and bots\n```{r data-processing}\n# Meshing time and computing the number of visits per mesh for internet users and bots\nbegining_time <- min(c(min(my_google_numeric_vector),min(my_internautes_numeric_vector)))\nending_time <- max(c(max(my_google_numeric_vector),max(my_internautes_numeric_vector)))\nmy_time_meshing_dimension <- max(diff(my_google_numeric_vector))/6\n# we go for 100 time periods\nmesh_beginning_time=begining_time\ni<-1\nmy_google_bot_number_of_passings=c()\nmy_internautes_number_of_passings=c()\nwhile(mesh_beginning_time <= ending_time)\n{\n  mesh_beginning_time <- begining_time+(i-1)*my_time_meshing_dimension\n  #print(mesh_beginning_time)\n  mesh_ending_time <- begining_time+i*my_time_meshing_dimension\n  #print(mesh_ending_time)\n  my_google_bot_number_of_passing = sum(((my_google_numeric_vector >=  mesh_beginning_time) & (my_google_numeric_vector <=  mesh_ending_time)))\n  print(\"Bot passings :\")\n  print(my_google_bot_number_of_passing)\n  my_google_bot_number_of_passings<-c(my_google_bot_number_of_passing,my_google_bot_number_of_passings)\n  print(\"Internautes passings\")\n  my_internautes_number_of_passing = sum(((my_internautes_numeric_vector >=  mesh_beginning_time) & (my_internautes_numeric_vector <=  mesh_ending_time)))\n  print(my_internautes_number_of_passing)\n  my_internautes_number_of_passings<-c(my_internautes_number_of_passing,my_internautes_number_of_passings)\n  i<-i+1\n}\n```\n\n\n### VISUALIZING BOTH SERIES ON A YY PLOT\n```{r data-visualization}\nrequire(pracma)\nmy_series_df <- data.frame(Internautes =my_internautes_number_of_passings, Bots = my_google_bot_number_of_passings, Time_Periods = 1:NROW(my_google_bot_number_of_passings) )\nplotyy(my_series_df$Time_Periods,my_series_df$Internautes, my_series_df$Time_Periods, my_series_df$Bots, main = \"Internet Users / Google Bot\")\n```\n\n### FORECASTING THE HOURLY NUMBER OF VISITS FOR THE SOLDES URL\n```{r data-model}\n# trying here to predict the crawling time hourly frequency hour by hour\nmy_frequency_vector <-ts(my_series_df$Bots)\n\n# Backtesting the one next day frequency for  hourly frequency hour by hour for a single URL\nbegining_times = 10 ;\narima_prediction = c(1:length(my_frequency_vector))\narima_prediction[1:begining_times]<- my_frequency_vector[1:begining_times]\n\nconstant_prediction = c(1:length(my_frequency_vector))\nconstant_prediction[1:begining_times]<- my_frequency_vector[1:begining_times]\n\nlinear_prediction = c(1:length(my_frequency_vector))\nlinear_prediction[1:begining_times]<- my_frequency_vector[1:begining_times]\n\nfor(i in begining_times:length(my_frequency_vector)) {\n  training_data<-my_frequency_vector[1:i]\n  feature = 1:i\n  # linear prediction\n  new_data <- data.frame(feature = 1:(NROW(training_data)+1))\n  my_linear_prediction<-predict(lm(training_data ~ feature), new_data, se.fit = TRUE)\n  #plot(my_linear_prediction$fit,training_data )\n  #par(new=T)\n  #plot(training_data )\n  \n  # ARIMA prediction\n  model <- arima(training_data, order=c(1,2,1)) \n  next_day_prediction <- predict(model, n.ahead=1)\n\n  # filling up the forecasting series\n  arima_prediction[i] <- as.integer(next_day_prediction$pred[1])\n  constant_prediction[i]=mean(training_data)\n  linear_prediction[i] <- tail(my_linear_prediction$fit, n=1)\n}\n```\n\n\n```{r model-performance-plot,dev='png'}\nmy_results_df <- data.frame(ARIMA_Prediction =arima_prediction, Constant_Prediction = constant_prediction, Linear_Prediction = linear_prediction, Entry = my_frequency_vector, Time_Periods = 1:NROW(my_frequency_vector))\n\n# plotting the two time series together (only the points) for the arima modeling\nrequire(ggplot2)\ng <- ggplot(my_results_df, aes(x=Time_Periods, y=Entry))\ng + geom_smooth(method=\"lm\") + geom_point(aes(x=Time_Periods,y=Entry,color='real data')) + geom_point(aes(x=Time_Periods,y=ARIMA_Prediction,color='ARIMA forecast'))+ geom_point(aes(x=Time_Periods,y=Constant_Prediction,color='Constant forecast')) + geom_point(aes(x=Time_Periods,y=Linear_Prediction,color='Linear forecast'))\n```\n\n### computing the root mean squared error for each errors\n```{r rmse-model-performance}\n# computing the root mean squared error\narima_rmse_constant = sqrt( mean( (my_results_df$ARIMA_Prediction - my_results_df$Entry)^2, na.rm = TRUE) )\nsprintf(\"ARIMA RMSE for ARIMA model %f\", arima_rmse_constant)\n\nconstant_rmse_constant = sqrt( mean( (my_results_df$Constant_Prediction - my_results_df$Entry)^2, na.rm = TRUE) )\nsprintf(\"Constant RMSE for constant model %f\", constant_rmse_constant)\n\nlinear_rmse_constant = sqrt( mean( (my_results_df$Linear_Prediction - my_results_df$Entry)^2, na.rm = TRUE) )\nsprintf(\"Linear RMSE for linear model %f\", linear_rmse_constant)\n\n```\n\n```{r cross-correlation-bot-internet-users}\nmy_bot_serie = my_series_df$Bots\nmy_internet_users_serie = my_series_df$Internautes \n####acf(my_bot_serie)\n####acf(my_internet_users_serie)\nccf(my_internet_users_serie,my_bot_serie)\n\n```\n\n\n",
    "created" : 1436966679546.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "714654279",
    "id" : "7EFAE944",
    "lastKnownWriteTime" : 1436966926,
    "path" : "~/R/CRAWLER_FREQUENCY_FORECASTING/logs_forecasting_slides_single_url_soldes_hourly_visits_desktop.Rmd",
    "project_path" : "logs_forecasting_slides_single_url_soldes_hourly_visits_desktop.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_markdown"
}